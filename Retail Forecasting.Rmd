---
title: "Retail Forecasting Project"
author: "Brendan Octaviano"
date: "20/05/2022"
output: github_document
---

# Loading packages and data

```{r, warning = FALSE, message = FALSE}

# Loading packages

library(fpp3)
library(knitr)

# Use your student ID as the seed
set.seed(28785649)

myseries <- aus_retail %>%
  # Remove discontinued series
  filter(!(`Series ID` %in% c("A3349561R","A3349883F","A3349499L","A3349902A",
                        "A3349588R","A3349763L","A3349372C","A3349450X",
                        "A3349679W","A3349378T","A3349767W","A3349451A"))) %>%
  # Select a series at random
  filter(`Series ID` == sample(`Series ID`, 1))

```

# Statistical features of the original data

```{r}

# Plotting the data

original_plot <- myseries %>%
  autoplot(Turnover) +
  labs(title = "Takeaway Food Services",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw()

# gg_season plot

season_plot <- myseries %>%
  gg_season(Turnover) +
  labs(title = "Takeaway Food Services",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw()

# gg_subseries plot

series_plot <- myseries %>%
  gg_subseries(Turnover) +
  labs(title = "Takeaway Food Services",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw()

# ACF

acf_original <- myseries %>%
  ACF(Turnover) %>%
  autoplot()

```


```{r, echo = FALSE}

original_plot

```

Between 1982 and 2018, we can see that there has been a generally upward trend in the total turnover from takeaway food services in New South Wales. Analysing the time series plot, we can also see some cyclical behaviour. At the same time, there is evidence of heteroskedasticity due to the difference in variation at the beginning of the time series compared to the end. There is also potential seasonality, which we will be able to confirm in the next plot. 

```{r, echo = FALSE}

season_plot

```

The above time series seasonal plot confirms that there is seasonality in the data. Besides a few years, turnover for takeaway food service revenue in New South Wales tends to experience a drop in the first two months of the year, with the minimum turnover for most years occurring in February, then slowly increases with slight dips along the way, peaking in December.

```{r, echo = FALSE}

series_plot

```

The above seasonal subseries further confirms that there is seasonality in the data. Besides a few years, turnover of takeaway food service revenue in New South Wales tends to experience a drop in the first two months of the year, with the trough occurring in February, then slowly increases with slight dips along the way, peaking in December.

```{r, echo = FALSE}

acf_original

```

Finally, looking at the ACF, we can see there is clear evidence of autocorrelation, as each of the lags are significant. 

# Transformation

After exploring the statistical features of the data, it is important to apply certain changes to our data. Firstly, to deal with the heteroskedasticity that is present in the data, while also making our model more simple, we can look to apply a mathematical transformation. 

We can first look at the potential of a Box-Cox transformation:

```{r}

# Box-Cox 

# Ideal value of lambda

lambda_myseries <- myseries %>%
  features(Turnover,
           guerrero) %>%
  pull(lambda_guerrero)

lambda_myseries %>%
  kable()

```

Our value of lambda is 0.086, which is close enough to 0, hence we can use a log transformation instead.

```{r}

# Plotting our log transform

transformed_plot <- myseries %>%
  autoplot(log(Turnover)) +
  labs(title = "Takeaway Food Services - Log Transformed",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw()

transformed_plot

```

While our variance appears constant over time after performing a log transformation, our data is clearly not stationary still, therefore we can look to difference our data. First, we will look to seasonally difference our data. 

```{r}

# Seasonal differencing

myseries %>%
  features(log(Turnover),
           unitroot_nsdiffs)  %>%
  kable()

```

After completing a unit root test, this tells us to apply one seasonal difference, which after applying can be seen below.

```{r, warning = FALSE}

# Plot after applying log transformation and one seasonal difference

myseries %>%
  autoplot(log(Turnover) %>%
             difference(lag = 12, 
                        differences = 1)) +
  labs(title = "Takeaway Food Services",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw()

```

After applying our seasonal difference, our data appears to be stationary, however we will check to see if we need to apply a first order difference. 

```{r}

# First order differencing

myseries %>%
  features(log(Turnover) %>%
             difference(lag = 12, 
                        differences = 1),
           unitroot_kpss) %>%
  kable()

```

After performing a unit root test, we attain a p-value of 0.1. As a result, we can reject the null hypothesis that a unit root exists (or in other words that our data is not stationary), hence telling us that no further differencing is required. Below, we will visualise the final results of our transformations and differencing compared to our original data.

```{r, warning = FALSE, out.width = "50%"}

# Applying transformation and plotting

transformed_differenced_plot <- myseries %>%
  autoplot(log(Turnover) %>%
             difference(lag = 12, 
                        differences = 1)) +
  labs(title = "Takeaway Food Services - Transformed and Differenced",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw()

```

```{r, echo = FALSE, out.width = "50%", warning = FALSE}

# Plotting original and transformed + differenced side by side

original_plot

transformed_differenced_plot

```

# Shortlist of different ARIMA and ETS models

## ETS models

```{r, echo = FALSE}

# Transformed data to help determine ETS models

transformed_plot 

```

Looking firstly at our log transformed data will help us create a shortlist of potential ETS models. We can see here after performing a log transformation, the variance seems constant, meaning our data looks homoskedastic, and as a result our error and seasonality parameters on our ETS models can both be additive ("A"). In terms of the trend, we can see that the trend is linear, so an additive trend parameter ("A") may be appropriate. However, for long term forecasts, an additive trend tends to over-forecast, hence we can also consider damping the trend for our trend parameter ("Ad") to account for this. We will also let the ETS() function select the best model based on the set criteria to compare the hand-picked models.

As a result, we will end up with the following ETS models:

* **ETS(A, A, A)**
* **ETA(A, Ad, A)**
* **ETS()**

## ARIMA models

To determine a shortlist of potential ARIMA models, we can look at our the ACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function) of our transformed and differenced data.

```{r, warning = FALSE}

# Plotting ACF and PACF

myseries %>%
  gg_tsdisplay(log(Turnover) %>%
             difference(lag = 12, 
                        differences = 1),
             plot_type = "partial")

```

Looking at the ACF and PACF of our transformed and differenced data, we can see clearly that our ACF dies down in a sinusoidal matter, hence it would be ideal to choose our AR(p) parameters as opposed to MA(q). To determine the order of p (for our non-seaonal component), as well as P (for our seasonal component), we can look at our PACF. Firstly, we can see that our first significant seasonal spike is at lag 12, and lag 24 is barely past the blue line, therefore it can be disregarded as significant. As a result, for our seasonal component, we can choose P to be 1. Now, we can see there are significant spikes at time 1, 13 and 25, and a significant spike at lag 2, and potentially at lag 14. Since we have monthly data and have determined that our seasonal component P to be 1, setting our non-seasonal component p to be 2 will look after these lags. This is due to the fact that there is an interaction between the non-seasonal and seasonal component of our ARIMA model, so for example, lag 1 and 2 will interact lag 12 will interact to look after the significant spike at lags 13 and 14. Further, a constant is required since the data does not appear to be completely centered on 0.

Therefore, we can choose one potential ARIMA model to be as follows:

* **ARIMA(2, 0, 0)(1, 1, 0)[12]**

We can also include some variations of this, also including an MA(q) component, and similar to above, choosing ones that are selected by the ARIMA() function to be the ideal models (both using stepwise/greedy and a brute-force algorithm).

As a result, our shortlist of ARIMA models will be as follows:

* **ARIMA(2, 0, 0)(1, 1, 0)[12] w/drift**
* **ARIMA(2, 0, 1)(1, 1, 1)[12] w/drift**
* **ARIMA(2, 0, 1)(1, 1, 2)[12] w/drift**
* **ARIMA(2, 0, 2)(1, 1, 1)[12] w/drift**
* **ARIMA()**
*  **ARIMA(stepwise = FALSE, approx = FALSE)**

Now, we can combine our shortlisted ETS and ARIMA models to come up with a shortlist of potential models to forecast our data.

```{r}

# Creating a shortlist of potential ETS and ARIMA models

fit_shortlist <- myseries %>%
  filter(year(Month) < 2017) %>%
  model(
    ets_aaa = ETS(log(Turnover) ~ error("A") + trend("A") + season("A")),
    ets_aada = ETS(log(Turnover) ~ error("A") + trend("Ad") + season("A")),
    ets_auto = ETS(log(Turnover)),
    arima_200110 = ARIMA(log(Turnover) ~ 1 + pdq(2, 0, 0) + PDQ(1, 1, 0)),
    arima_201111 = ARIMA(log(Turnover) ~ 1 + pdq(2, 0, 1) + PDQ(1, 1, 1)),
    arima_201112 = ARIMA(log(Turnover) ~ 1 + pdq(2, 0, 1) + PDQ(1, 1, 2)),
    arima_202111 = ARIMA(log(Turnover) ~ 1 + pdq(2, 0, 2) + PDQ(1, 1, 1)),
    arima_auto = ARIMA(log(Turnover)),
    arima_best = ARIMA(log(Turnover), stepwise = FALSE, approx = FALSE)
  )

```

After creating a shortlist of models, we have applied our model to our *training data* (being all data in the dataset besides the last two years), and to compare models we can firstly look at the AIC.

```{r, warning = FALSE}

# Evaluating the AIC for ETS and ARIMA models

fit_shortlist %>%
  report() %>%
  arrange(-AIC) %>%
  select(-ar_roots, -ma_roots) %>%
  kable()

```

Comparing firstly our different ETS models, we can see that our *ETS(A, Ad, A)* model gives us the lowest AIC, implying it is a more parsimonious model than our other ETS models from our shortlist. In terms of our ARIMA models, we can see that our *ARIMA(2, 0, 0)(1, 1, 0)[12] w/drift* gives us the lowest AIC, which again implies it is a more parsimonious model than our other ARIMA models, however it is not far off from our other ARIMA models. Before concluding they are the best models out of each of the two classes of models, we can compare to see how they perform against our test set.

```{r, warning = FALSE}

# Evaluating accuracy metrics on 2 year forecasts for ETS and ARIMA models

fit_shortlist %>%
  forecast(h = 24) %>%
  accuracy(myseries) %>%
  arrange(RMSE) %>%
  kable()

```

After applying our models onto our training data, producing forecasts on our test data and comparing it to our actual data, we can see our above analysis is consistent with the metrics above (RMSE and MAE) for our ETS models, with an ETS(A, Ad, A) model giving us our lowest values of RMSE and MAE, however our analysis for our ARIMA models is inconsistent. 

Our automatically selected ARIMA model using the stepwise approach with the ARIMA() function gives us the best predictions on our test set, which is evident with the lowest RMSE and MAE overall, being an ARIMA(2,0,2)(0,1,1)[12] w/ drift. The ARIMA model that gave us the lowest AIC was ARIMA(2,0,0)(1,1,0)[12] w/ drift, which in fact gave us notably lower RMSE and MAE values. The AIC can tell us which model is a "better fit", but ultimately we need to find a balance between what model is a better fit, and what makes accurate predictions. The AIC values are not significantly different between the two ARIMA models, hence we can choose the ARIMA model with the significantly better RMSE - ARIMA(2,0,2)(0,1,1)[12] w/ drift. 

Therefore, the two models we will proceed with for our analysis are:

* **ETS(A, Ad, A)** 
* **ARIMA(2,0,2)(0,1,1)[12] w/ drift**

# Comparing optimal ETS and ARIMA model

```{r}

# Optimal models

fit_optimal <- myseries %>%
  filter(year(Month) < 2017) %>%
  model(
    arima_auto = ARIMA(log(Turnover)),
    ets_aada = ETS(log(Turnover) ~ error("A") + trend("Ad") + season("A"))
  )

```

## ETS

#### Parameter estimates

```{r}

# ETS

# Parameter estimates

fit_optimal %>%
  select(ets_aada) %>%
  report() 

```

#### Residual diagnostics

```{r}

# Residual diagnostics

fit_optimal %>%
  select(ets_aada) %>%
  gg_tsresiduals()

```

We can see there is some evidence of autocorrelation in the residuals of our ETS model, as there are some significant lags. We will confirm this with a Ljung-Box test further below.

#### Forecasts

```{r}

# Plotting 2 year forecasts for ETS model

fit_optimal %>%
  forecast(h = 24) %>%
  filter(.model == "ets_aada") %>%
  autoplot(myseries %>%
             filter(year(Month) < 2017)) +
  labs(title = "Takeaway Food Services - Two Year Forecast (ETS)",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw()

```

#### Ljung-Box Test

```{r}

# Peforming Ljung-Box Test

fit_optimal %>%
  augment() %>%
  filter(.model == "ets_aada") %>%
  features(.innov, 
           ljung_box, 
           lag = 24, 
           # 17 degrees of freedom since we have 18 coefficients, must subtract 1 for an ETS model
           dof = 17) %>%
  kable()

```

After performing a Ljung-Box test, we attain a p-value of **0.0015792**. At a 95% level of significance, we can reject the null hypothesis that residuals are white noise (i.i.d, with mean 0). However, looking at the ACF, we can see that the lags are only slightly significant, only just passing the blue lines at lags 10 and 21. In any case, it is important to note that because we cannot say that the residuals are white noise the our forecast may be biased, despite the lags only just passing the blue lines.

## ARIMA

#### Parameter estimates

```{r}

# ARIMA

# Parameter estimates

fit_optimal %>%
  select(arima_auto) %>%
  report() 

```

#### Residual diagnostics

```{r}

# Residual diagnostics

fit_optimal %>%
  select(arima_auto) %>%
  gg_tsresiduals()

```

There is only one significant spike at lag 21, however this is 1 out of 24 lags, which is expected since we are testing a large number of lags. To confirm our residuals are behaving as desired, we can perform a Ljung-Box test, which is done so further below.

#### Forecasts

```{r}

# Plotting 2 year forecasts for ARIMA model

fit_optimal %>%
  forecast(h = 24) %>%
  filter(.model == "arima_auto") %>%
  autoplot(myseries %>%
             filter(year(Month) < 2017)) +
  labs(title = "Takeaway Food Services - Two Year Forecast (ARIMA)",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw()

```

#### Ljung-Box Test

```{r}

# Performing Ljung-Box Test

fit_optimal %>%
  augment() %>%
  filter(.model == "arima_auto") %>%
  features(.innov, 
           ljung_box, 
           lag = 24, 
           dof = 6) %>%
  kable()

```

After performing a Ljung-Box test, we attain a p-value of **0.2014686**. At a 95% level of significance, we fail to reject the null hypothesis that residuals are white noise (i.i.d, with mean 0). This is the ideal situation, as one of the assumptions of our forecasts are that our residuals are white noise.

# Comparison of results from both models

Looking at the above analysis, it is quite clear that our results indicate that the *ARIMA model would be a more optimal model than our ETS model*. 

```{r}

# Accuracy metrics on 2 year forecasts for ETS and ARIMA models

fit_optimal %>%
  forecast(h = 24) %>%
  accuracy(myseries) %>%
  arrange(RMSE) %>%
  kable()

```

Firstly, the RMSE and MAE values of our ARIMA model are much smaller than our ETS model, indicating it makes much better predictions on our test data (or in other words, data it hasn't seen). On top of this, the residuals of our ETS model appear to show evidence of autocorrelation, confirmed by the Ljung-Box test we have performed, meaning that we cannot completely trust our forecasts from the ETS model.

# Applying models to full data set and producing out-of-sample forecasts

```{r}

# Fitting models

fit_full_optimal <- myseries %>%
  model(
    arima_auto = ARIMA(log(Turnover)),
    ets_aada = ETS(log(Turnover) ~ error("A") + trend("Ad") + season("A"))
  )

# Producing out of sample forecasts for two years

# Forecasts

optimal_full_forecasts <- fit_full_optimal %>%
  forecast(h = 24) 

```

## ETS

#### 80% Prediction intervals

```{r}

# Prediction intervals

optimal_full_forecasts %>%
  hilo(level = 80) %>% 
  filter(.model == "ets_aada") %>%
  select(Month, Turnover, .mean, `80%`) %>%
  kable()

```

#### Plotting forecasts

```{r}

# Plotting 2 year forecasts on ETS model

optimal_full_forecasts %>%
  filter(.model == "ets_aada") %>%
  autoplot(myseries,
           level = 80) +
  labs(title = "Takeaway Food Services - Two Year Out-of-Sample Forecast (ETS)",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw() 

```

## ARIMA

#### 80% Prediction intervals

```{r}

# Prediction intervals

optimal_full_forecasts %>%
  hilo(level = 80) %>% 
  filter(.model == "arima_auto") %>%
  select(Month, Turnover, .mean, `80%`) %>%
  kable()

```

##### Plotting forecasts

```{r}

# Plotting 2 year forecasts on ETS model

optimal_full_forecasts %>%
  filter(.model == "arima_auto") %>%
  autoplot(myseries,
           level = 80) +
  labs(title = "Takeaway Food Services - Two Year Out-of-Sample Forecast (ARIMA)",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw()


```

# Comparing with real data from Australian Bureau of Statistics (ABS)

To see how well our models perform on out-of-sample data, we can obtain real data from the ABS website and compare how accurate our forecasts are.

```{r, warning = FALSE, message = FALSE}

# Loading package necessary to load data from ABS

library(readabs)

# Loading data data for New South Wales Takeaway Services

real_abs_data <- read_abs(series_id = "A3349792X")

# Selecting only necessary columns and filtering out data we did not forecast

real_abs_data_clean <- real_abs_data %>%
  mutate(Month = yearmonth(format(as.Date(date), "%Y-%m")),
         Turnover = value) %>%
  filter(year(Month) < 2021) %>%
  select(Month, Turnover) %>%
  mutate(State = rep("New South Wales", nrow(real_abs_data) - 16),
                     Industry = rep("Takeaway food services", nrow(real_abs_data) - 16)) %>%
  as_tsibble(key = c(State, Industry),
             index = Month)


```

## Plotting real data

#### ETS

```{r}

# Whole dataset

optimal_full_forecasts %>%
  filter(.model == "ets_aada") %>%
  autoplot(real_abs_data_clean,
           level = 80) +
  labs(title = "Takeaway Food Services - Two Year Out-of-Sample Forecast (ETS)",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw()

# Just forecasts

optimal_full_forecasts %>%
  filter(.model == "ets_aada") %>%
  autoplot(real_abs_data_clean %>%
             filter(year(Month) > 2018),
           level = 80) +
  labs(title = "Takeaway Food Services - Two Year Out-of-Sample Forecast (ETS) \n(Forecasts Only)",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw() 

```

#### ARIMA

```{r}

# Whole dataset

optimal_full_forecasts %>%
  filter(.model == "arima_auto") %>%
  autoplot(real_abs_data_clean,
           level = 80) +
  labs(title = "Takeaway Food Services - Two Year Out-of-Sample Forecast (ARIMA)",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw()

# Just forecasts

optimal_full_forecasts %>%
  filter(.model == "arima_auto") %>%
  autoplot(real_abs_data_clean %>%
             filter(year(Month) > 2018),
           level = 80) +
  labs(title = "Takeaway Food Services - Two Year Out-of-Sample Forecast (ARIMA) \n(Forecasts Only)",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw() 

```

## Computing statistics to evaluate forecasts

```{r}

optimal_full_forecasts %>%
  accuracy(real_abs_data_clean) %>%
  kable()

```

```{r, echo = FALSE, out.width = "50%"}

optimal_full_forecasts %>%
  filter(.model == "ets_aada") %>%
  autoplot(real_abs_data_clean %>%
             filter(year(Month) > 2018),
           level = 80) +
  labs(title = "Takeaway Food Services - Two Year Out-of-Sample Forecast (ETS) \n(Forecasts Only)",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw() 

optimal_full_forecasts %>%
  filter(.model == "arima_auto") %>%
  autoplot(real_abs_data_clean %>%
             filter(year(Month) > 2018),
           level = 80) +
  labs(title = "Takeaway Food Services - Two Year Out-of-Sample Forecast (ARIMA) \n(Forecasts Only)",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw() 

```

Visually, we can see that both our ARIMA and ETS models did fairly well, with the majority of our predictions falling within our 80% prediction intervals. Unfortunately, an unpredictable situation like COVID-19 was the cause of the massive drop, but this it something no one saw coming, and as a result is hard to predict for.

Our **ARIMA(2,0,2)(0,1,1)[12] w/ drift** model produced a lower RMSE than our **ETS(A, Ad, A)** model, however had a higher MAE. RMSE tends to penalise larger errors more than MAE, and looking to minimise the RMSE looks to provide forecasts of the mean. On the other hand, looking to minimise MAE looks to provide forecasts of the median. Plotting a histogram of our original data, we can see that the data is asymmetrical, and looking to provide forecasts of the median by minimising the MAE does not make sense, hence the RMSE would be a better measure in this case, as instead it can indicate which model provides the most unbiased forecast.

```{r, message = FALSE, error = FALSE}

# Histogram of original data

myseries %>%
  ggplot(aes(x = Turnover)) +
  geom_histogram() +
  labs(title = "Takeaway Food Services (Histogram)",
       subtitle = "New South Wales",
       x = "Turnover [$million AUD]") +
theme_bw()

```

As a result, since our **ARIMA(2,0,2)(0,1,1)[12] w/ drift** model produced a lower RMSE then our **ETS(A, Ad, A)** model, we can say that our ARIMA model performed better with an out-of-sample forecast.

```{r, echo = FALSE}

# ARIMA forecast on ABS data

optimal_full_forecasts %>%
  filter(.model == "arima_auto") %>%
  autoplot(real_abs_data_clean %>%
             filter(year(Month) > 2018),
           level = 80) +
  labs(title = "Takeaway Food Services - Two Year Out-of-Sample Forecast (ARIMA) \n(Forecasts Only)",
       subtitle = "New South Wales",
       y = "Turnover [$million AUD]") +
  theme_bw() 

```

# Discussion of benefits and limitations of the chosen models on data

Both ETS and ARIMA models that were chosen performed fairly well on our data, however both types of models have their own benefits and limitations. Firstly, ETS models can handle non-stationary data. This is useful because it is quite rare to find data that is stationary without any transformations or differencing, *like our turnover data for takeaway food services in New South Wales*, hence ETS models may be a convenient option. At the same time, mathematical transformations are not necessary. While they were applied above (applying a log transformation), it was possible to simply apply an ETS(M, Ad, M) model, which is equivalent to applying a log or Box-Cox transformation, then fitting an ETS(A, Ad, A) model.

On the other hand, ARIMA models handle stationary data, so one limitation is that data must be stationary before being able to fit an ARIMA model to data, which requires differencing. However, one benefit to ARIMA models is that it can be quite useful when autocorrelation exists in your data (which is the AR (autoregressive) component of ARIMA), or even in the residuals (which is the MA (moving average) component of ARIMA), hence it was useful in our *our turnover data for takeaway food services in New South Wales* since there was clear evidence of autocorrelation. Moreover, our ARIMA model had significantly less coefficients than the ETS model, meaning it was a simpler and more efficient model, providing better forecasts with less parameters.